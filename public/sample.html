<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>

  <link rel="stylesheet" href = "css/styles.css">


  <script type="module">

    // Step 1: Load HTML elements to JS Variables
      const videoElement = document.getElementsByClassName('input_video')[0];
      const canvasElement = document.getElementsByClassName('output_canvas')[0];
      const canvasCtx = canvasElement.getContext('2d');  
      const labelElement = document.getElementById("label"); 
      const modelButton = document.getElementById('model-change');   
      const modelName = document.getElementById('model-name');

    // Step 2: Intialize Variables for Model and labels
      let alphabetLabels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'U', 'V', 'W', 'X', 'Y']
      var model;
      let label = '';


    // Step 3: Initialize static functions used for calculation
      function get_angles(a,b,c){
        ang = (Math.atan2(c[1]-b[1], c[0]-b[0]) - Math.atan2(a[1]-b[1], a[0]-b[0])) * (180/Math.PI);
        if(ang<0)
          return 360 + ang;
        else
          return ang;                                                 
      }

      function indexOfMax(arr) {
        if (arr.length === 0) {
            return -1;
        }
        var max = arr[0];
        var maxIndex = 0;
        for (var i = 1; i < arr.length; i++) {
            if (arr[i] > max) {
                maxIndex = i;
                max = arr[i];
            }
        }
        return maxIndex;
    }

    modelButton.addEventListener('click', function(){
      if(modelName.textContent == "Alphabet Model")
        modelName.textContent = "Gesture Model";
      else
        modelName.textContent = "Alphabet Model";
    });


    // Step 4: Start loading the model asynchronously   
      console.log("Start Load");

      async function loadModel(){     // Promise of loading model from JSON file
        const model = await tf.loadLayersModel('model.json')
        return model;
      }

      let loadedModel = loadModel().then((resolve, reject)=>{   //Handing the promise from before
        model = resolve;
      }).then(function() {
          console.log("Model: ", model) 
      });


    // Step 5: Async prediction function which predicts the probabilities of classes and return label
      async function predictModel(input){
        console.log("Input is: ", input.arraySync());
        const predictionArr = await model.predict(input);
        const prediction =  indexOfMax(predictionArr.arraySync()[0]);
        console.log("ArgMax: ", prediction);

        return alphabetLabels[prediction];
      }

    // Step 6: The required onResults function for the Mediapipe model. Collect landmarks and
    // format them as 3D tensors before passing it to the predct function   
      function onResults(results) {

        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        canvasCtx.drawImage(
            results.image, 0, 0, canvasElement.width, canvasElement.height);

        if (results.multiHandLandmarks) {

          const landmark_list =[];
          for (const landmarks of results.multiHandLandmarks) {

              for(let i=0; i<21; i++){
                const temp = [landmarks[i].x * 480, landmarks[i].y * 640] //Adjust Aspect ratio
                landmark_list.push(temp);
              }

              drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS,
                            {color: '#00FF00', lineWidth: 5});
              drawLandmarks(canvasCtx, landmarks, {color: '#FF0000', lineWidth: 2});
              
              console.log(tf.tensor([landmark_list]));

              let predictionMade = predictModel(tf.tensor([landmark_list])).then((resolve, reject)=>{
                label = resolve;
                labelElement.textContent = label;
                console.log("Promise Output: ", resolve, reject);
              }).then(function() {
                  console.log("Answer: ", label);
              });

          }
        }
        canvasCtx.restore();
      }
      

      const hands = new Hands({locateFile: (file) => {
        return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
      }});


      hands.setOptions({
        maxNumHands: 2,
        modelComplexity: 1,
        minDetectionConfidence: 0.7,
        minTrackingConfidence: 0.7
      });
      hands.onResults(onResults);
      
      const camera = new Camera(videoElement, {
        onFrame: async () => {
          await hands.send({image: videoElement});
        },
        width: 1280,
        height: 720
      });
      camera.start();

  </script>

</head>

<body>
  <h1>Media Pipe Demo</h1>
  <h2 id="label">Label Here</h1>

  <div class="container">
    <video class="input_video"></video>
    <canvas class="output_canvas" width="640px" height="480px"></canvas>
    <div id="model-controls">
        <button id="model-change">Change Model</button>
        <h3 id="model-name">Current Model</h3>
    </div>
    
  </div>
</body>
</html>


